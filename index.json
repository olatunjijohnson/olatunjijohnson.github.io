[{"authors":["admin"],"categories":null,"content":"Olatunji Johnson is a post-doctoral researcher at CHICAS Research Group, Lancaster Medical School, Lancaster University, UK. He was formerly a PhD student at Lancaster University and supervised by Prof. Peter Diggle, Dr Emanuele Giorgi and Prof. Jo Knight. His research focuses on the development of novel geospatial statistical methodology for analysing epidemiological data, currently working on Neglected Tropical Diseases (NTDs) in low resource countries. He is the author of SDALGCP R package.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1587077318,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Olatunji Johnson is a post-doctoral researcher at CHICAS Research Group, Lancaster Medical School, Lancaster University, UK. He was formerly a PhD student at Lancaster University and supervised by Prof. Peter Diggle, Dr Emanuele Giorgi and Prof.","tags":null,"title":"","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1588353970,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588353970,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588353970,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":"       Introduction This post briefly describe how to produce a multiple axes plot in R using plotly. Happy reading!!!\nGenerate the Data\ncountM \u0026lt;-rpois(n = 26, lambda = 10) countF \u0026lt;-rpois(n = 26, lambda = 10) rateM \u0026lt;- countM/1000 rateF \u0026lt;- countF/1000 age \u0026lt;- LETTERS[seq( from = 1, to = 26 )] data \u0026lt;- data.frame(countM, countF, rateM, rateF, age) Load the packages\nrequire(tidyverse) require(ggplot2) require(plotly) Create the Multiple Y Axes\n#create ay \u0026lt;- list( tickfont = list(color = \u0026quot;red\u0026quot;), overlaying = \u0026quot;y\u0026quot;, side = \u0026quot;right\u0026quot;, title = \u0026quot;Rate\u0026quot; ) l \u0026lt;- list( font = list( family = \u0026quot;sans-serif\u0026quot;, size = 12, color = \u0026quot;#000\u0026quot;), bgcolor = \u0026quot;#E2E2E2\u0026quot;, bordercolor = \u0026quot;#FFFFFF\u0026quot;, borderwidth = 2, orientation= \u0026quot;h\u0026quot;, x = 1.05, y = 1) p \u0026lt;- plot_ly(data, x = ~age, y = ~countM, type = \u0026#39;bar\u0026#39;, name = \u0026#39;Male\u0026#39;) %\u0026gt;% add_trace(y = ~countF, name = \u0026#39;female\u0026#39;) %\u0026gt;% add_lines(x=~age, y=~rateM, name = \u0026quot;rate of Male\u0026quot;, yaxis = \u0026quot;y2\u0026quot;) %\u0026gt;% add_lines(x=~age, y=~rateF, name = \u0026quot;rate of Female\u0026quot;, yaxis = \u0026quot;y2\u0026quot;) %\u0026gt;% layout(title = \u0026#39;Age and Sex distribution\u0026#39;, yaxis = list(title = \u0026#39;count\u0026#39;), xaxis= list(title=\u0026quot;Age\u0026quot;), yaxis2 = ay, barmode = \u0026#39;group\u0026#39;, bargap = 0.15, legend= l) p  {\"x\":{\"visdat\":{\"5f7b73f31073\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"5f7b73f31073\",\"attrs\":{\"5f7b73f31073\":{\"x\":{},\"y\":{},\"name\":\"Male\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"bar\"},\"5f7b73f31073.1\":{\"x\":{},\"y\":{},\"name\":\"female\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"bar\",\"inherit\":true},\"5f7b73f31073.2\":{\"x\":{},\"y\":{},\"name\":\"rate of Male\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"mode\":\"lines\",\"yaxis\":\"y2\",\"inherit\":true},\"5f7b73f31073.3\":{\"x\":{},\"y\":{},\"name\":\"rate of Female\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"mode\":\"lines\",\"yaxis\":\"y2\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"title\":\"Age and Sex distribution\",\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"count\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Age\",\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]},\"yaxis2\":{\"tickfont\":{\"color\":\"red\"},\"overlaying\":\"y\",\"side\":\"right\",\"title\":\"Rate\"},\"barmode\":\"group\",\"bargap\":0.15,\"legend\":{\"font\":{\"family\":\"sans-serif\",\"size\":12,\"color\":\"#000\"},\"bgcolor\":\"#E2E2E2\",\"bordercolor\":\"#FFFFFF\",\"borderwidth\":2,\"orientation\":\"h\",\"x\":1.05,\"y\":1},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\"y\":[9,10,11,12,8,12,8,8,7,8,7,10,12,10,14,11,8,14,10,14,7,11,9,12,14,6],\"name\":\"Male\",\"type\":\"bar\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\"y\":[7,8,11,13,7,7,9,9,14,10,12,7,9,5,9,11,11,7,11,15,18,10,10,6,12,8],\"name\":\"female\",\"type\":\"bar\",\"marker\":{\"color\":\"rgba(255,127,14,1)\",\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\"y\":[0.009,0.01,0.011,0.012,0.008,0.012,0.008,0.008,0.007,0.008,0.007,0.01,0.012,0.01,0.014,0.011,0.008,0.014,0.01,0.014,0.007,0.011,0.009,0.012,0.014,0.006],\"name\":\"rate of Male\",\"type\":\"scatter\",\"mode\":\"lines\",\"yaxis\":\"y2\",\"marker\":{\"color\":\"rgba(44,160,44,1)\",\"line\":{\"color\":\"rgba(44,160,44,1)\"}},\"error_y\":{\"color\":\"rgba(44,160,44,1)\"},\"error_x\":{\"color\":\"rgba(44,160,44,1)\"},\"line\":{\"color\":\"rgba(44,160,44,1)\"},\"xaxis\":\"x\",\"frame\":null},{\"x\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\"y\":[0.007,0.008,0.011,0.013,0.007,0.007,0.009,0.009,0.014,0.01,0.012,0.007,0.009,0.005,0.009,0.011,0.011,0.007,0.011,0.015,0.018,0.01,0.01,0.006,0.012,0.008],\"name\":\"rate of Female\",\"type\":\"scatter\",\"mode\":\"lines\",\"yaxis\":\"y2\",\"marker\":{\"color\":\"rgba(214,39,40,1)\",\"line\":{\"color\":\"rgba(214,39,40,1)\"}},\"error_y\":{\"color\":\"rgba(214,39,40,1)\"},\"error_x\":{\"color\":\"rgba(214,39,40,1)\"},\"line\":{\"color\":\"rgba(214,39,40,1)\"},\"xaxis\":\"x\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} You can create a link to your chart\n# Create a shareable link to your chart # Set up API credentials: https://plot.ly/r/getting-started # chart_link = api_create(p, filename=\u0026quot;multiple-y-axes\u0026quot;) # chart_link  ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"e6fa012c6e08af4532055effaad5b8f3","permalink":"/post/plotly/plotly/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/post/plotly/plotly/","section":"post","summary":"Introduction This post briefly describe how to produce a multiple axes plot in R using plotly. Happy reading!!!\nGenerate the Data\ncountM \u0026lt;-rpois(n = 26, lambda = 10) countF \u0026lt;-rpois(n = 26, lambda = 10) rateM \u0026lt;- countM/1000 rateF \u0026lt;- countF/1000 age \u0026lt;- LETTERS[seq( from = 1, to = 26 )] data \u0026lt;- data.","tags":null,"title":"Multiple Y Axes Plot with Plotly","type":"post"},{"authors":["O Johnson","P Diggle","E Giorgi"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"555805b7d78f395caed8fc67fef00fbc","permalink":"/publication/misalignment/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/misalignment/","section":"publication","summary":"Life expectancy at birth (LEB), one of the main indicators of human longevity, has often been used to characterise the health status of a population. Understanding its relationships with the deprivation is key to develop policies and evaluate interventions that are aimed at reducing health inequalities. However, methodological challenges in the analysis of LEB data arise from the fact that different Government agencies often provide spatially aggregated information on LEB and the index of multiple deprivation (IMD) at different spatial scales. Our objective is to develop a geostatistical framework that, unlike existing methods of inference, allows to carry out spatially continuous prediction while dealing with spatial misalignment of the areal-level data. We developed a model-based geostatistical approach for the joint analysis of LEB and IMD, when these are available over different partitions of the study region. We model the spatial correlation in LEB and IMD across the areal units using inter-point distances based on a regular grid covering the whole of the study area. The advantages and strengths of the new methodology are illustrated through an analysis of LEB and IMD data from the Liverpool district council. We found that the effect of IMD on LEB is stronger in males than in females, explaining about 63.35% of the spatial variation in LEB in the former group and 38.92% in the latter. We also estimate that LEB is about 8.5 years lower between the most and least deprived area of Liverpool for men, and 7.1 years for women. Finally, we find that LEB, both in males and females, is at least 80% likely to be above the England wide average only in some areas falling in the electoral wards of Childwall, Woolton and Church. The novel model-based geostatistical framework provides a feasible solution to the spatial misalignment problem. More importantly, the proposed methodology has the following advantages over existing methods used model LEB: (1) it can deliver spatially continuous inferences using spatially aggregated data; (2) it can be applied to any form of misalignment with information provided at a range of spatial scales, from areal-level to pixel-level.","tags":[],"title":"Dealing with spatial misalignment to model the relationship between deprivation and life expectancy: a model-based geostatistical approach","type":"publication"},{"authors":[],"categories":null,"content":"","date":1580302800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580302800,"objectID":"80dbfb9d8de088dac4fe86a45016e51c","permalink":"/talk/capetown/","publishdate":"2020-01-28T13:00:00Z","relpermalink":"/talk/capetown/","section":"talk","summary":"In this talk, I will discuss statistical models used in disease mapping. Spatial statistics is classified into three categories according to Cressie’s (1991) book, namely geostatistics, lattice data, point patterns. I will discuss the distinction between the three classes and show that in many public health applications the main distinction is according to the underlying process. I will give three different applications. The first is on geostatistical analysis of River blindness in Cameroon. This study was used to help national control pro- gram to define hotspot areas, adjust treatment boundaries, prioritize areas with urgent Community-directed Treatments with Ivermectin (CDTI) and evaluate the progress towards elimination. The second is on disease risk mapping of chronic obstructive pulmonary disease (COPD) emergency admission. I will introduce a new methodology we developed to carry our spatially continuous prediction of COPD emergency admission risk when the data is available on an area level. The third application is on modelling the relationship between life expectancy at birth (LEB) and index of multiple deprivation (IMD) when they are spatially misaligned. I will give an overview of our proposed model-based geostatistical approach to solving spatial misalignment problem.","tags":[],"title":"Statistical Modelling Approaches to Disease Mapping","type":"talk"},{"authors":["OO Johnson","PJ Diggle","E Giorgi"],"categories":null,"content":"","date":1561935600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561935600,"objectID":"aba020ef41d973ce48d08f165b1eb58f","permalink":"/publication/sdalgcp/","publishdate":"2019-07-01T00:00:00+01:00","relpermalink":"/publication/sdalgcp/","section":"publication","summary":"In this paper, we develop a computationally efficient discrete approximation to log-Gaussian Cox process (LGCP) models for the analysis of spatially aggregated disease count data. Our approach overcomes an inherent limitation of spatial models based on Markov structures, namely that each such model is tied to a specific partition of the study area, and allows for spatially continuous prediction. We compare the predictive performance of our modelling approach with LGCP through a simulation study and an application to primary biliary cirrhosis incidence data in Newcastle-Upon-Tyne, UK. Our results suggest that when disease risk is assumed to be a spatially continuous process, the proposed approximation to LGCP provides reliable estimates of disease risk both on spatially continuous and aggregated scales. The proposed methodology is implemented in the open-source R package SDALGCP.","tags":[],"title":"A Spatially Discrete Approximation to Log-Gaussian Cox Processes for Modelling Aggregated Disease Count Data","type":"publication"},{"authors":null,"categories":null,"content":" Introduction Spatial probit models is very popular in spatial econometrics and the book of J. LeSage and Pace (2009) gives a very good overview. This is basically an extension of probit model when one is interested to adjust for both fixed and spatial random effect. J. P. LeSage (2000) model the spatial random effect using the class of Gaussian Markov Random Field (GMRF) introduced by Besag, York, and Mollié (1991) and provides a Bayesian procedure for fitting this class of model. Smith and LeSage (2004) further extends the probit model to allow for spatial random effect and individual random effect.\nMost of the extension to allow for spatial dependence structure utilises Simultatenous Autoregressive model (SAR) and Conditional autoregressive model (CAR) - a class of Gaussian markov random field (GMRF). The use of Guassian Random Field (GFR) is not popular for probit model, infact we cannot find any in literature. It was pointed out in Brooks et al. (2011) that estimating the covariance parameter of the GRF in a spatial probit model suffers from identifiability issues. And noted that a binary outcome contain less information about the magnitude of dependence. Also suggest using GMRF because “it aggregates pieces of binary information from the neigbouring regions to better estimate spatial depencies”. However, this is true because the the spatial dependence parameter is linear in the linear predictor and there exist a nice framework proposed by Besag, York, and Mollié (1991) that allows for this. It is important to note that there are several debates on the interpretation of the spatial dependence paramter of a CAR or SAR model. By using GMRF models, an explicit assumption that is made is that the spatial dependence parameter reflects a situation where values observed at one region, say observation \\(i\\), depend on the values of neighboring observations at nearby regions. And using a GRF, we basically assume that there is a spatial continuous process that generate any observation at any region.\n Methodological Framework Let \\(Y_{ijk}\\) be the binary outcome for an admission \\(i= 1, \\ldots, L\\) for patient \\(k= 1, \\ldots, N\\) living in region \\(j= 1, \\ldots, M\\). A augumented probit model proposed by Albert and Chib (1993) usually assume that the outcome is generated through an underlying latent variable, $Z_{ijk}, with the following hierarchical structure.\n\\[ Y_{ijk} = \\begin{cases} 1 \u0026amp; Z_{ijk} \u0026gt; 0 \\\\ 0 \u0026amp; Z_{ijk} \\leq 0 \\end{cases} \\]\n\\[Z_{ijk} = d_{ijk}^\\top \\beta + S_j + U_{k} + \\epsilon_{ijk}\\]  with the following prior distributions\n\\[\\epsilon_{ijk} \\sim \\mathcal{N}(0, 1)\\]\n\\[\\beta \\sim \\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)\\]\n\\[S \\sim \\mathcal{N}(0, \\Sigma(\\theta))\\]\n\\[Z \\sim \\mathcal{N}(0, \\tau^2I) \\]\nwhere \\(d_{ijk}\\) is the vector of \\(p\\) covariates with associated coefficient \\(\\beta\\), \\(S_j\\) is the region specific random effect and it is define using GRF and \\(U_{k}\\) is an patient or individual random effect, \\(\\epsilon_{ijk}\\) is the error term.\ncan be re-written in vector form as  \\[Z = D^\\top \\beta + \\Delta_1S + \\Delta_2 U + \\epsilon\\] where \\(Z=(Z_{ijk}: i = 1, \\ldots, L, j= 1, \\ldots, M, k = 1, \\ldots, N)\u0026#39;\\), \\(S=(S_{j}: j= 1, \\ldots, M)\u0026#39;\\), \\(U=(U_{j}: j= 1, \\ldots, U)\u0026#39;\\), \\[ \\Delta_1 = \\begin{pmatrix} 1 \u0026amp; \u0026amp; \\\\ \u0026amp;\\ddots \u0026amp;\\\\ \u0026amp; \u0026amp; 1 \\end{pmatrix}\\]\nThe joint posterior distribution of \\(\\beta\\), \\(\\theta\\), \\(\\tau^2\\), \\(S\\), \\(U\\) and \\(Z\\) is given by,\n2 \\[ \\label{eq:post} p(\\beta, \\theta, \\tau^2, S, U, Z | Y) = \\pi(\\beta) \\pi(\\theta) \\pi(\\tau^2) f(S| \\theta) f(U | \\tau^2) f(Z | S, U, \\beta, \\theta, \\tau^2) f(Y|Z) \\]\nWe use MCMC algorithm to update these parameters in turn\n Initialise \\(\\beta\\), \\(\\theta\\), \\(\\tau^2\\), \\(S\\), \\(U\\) and \\(Z\\) Update joint posterior of \\(\\theta\\) using random-walk Metropolis Hastings (RWMH) Update joint posterior of \\(\\tau\\) using random-walk Metropolis Hastings (RWMH) Update joint posterior of \\(\\beta\\) using a Gibbs step. The posterior distribution for \\(\\beta\\) is Gaussian since we assign a conjugate prior on \\(\\beta\\), hence \\[ [\\beta| \\cdot] = \\mathcal{N}(M_\\beta, V_\\beta),\\] where \\[V_\\beta = (D^\\top D + \\Sigma_\\beta^{-1})^{-1} \\] and \\[M_\\beta = V_\\beta (D^\\top (Z- (S+U)) + \\Sigma_\\beta^{-1} \\mu_\\beta) \\] Update joint posterior of \\(S\\) using a Gibbs step. The posterior distribution for \\(S\\) is Gaussian since we assign a conjugate prior on \\(S\\), hence \\[ [S| \\cdot] \\sim \\mathcal{N}(M_S, V_S),\\] where \\[V_S = (\\Delta_1^\\top \\Delta_1 + \\Sigma_S^{-1})^{-1} \\] and \\[M_S = V_S (\\Delta_1^\\top (Z- (D\\beta+ \\Delta_2U))) \\] Update joint posterior of \\(U\\) using a Gibbs step. The posterior distribution for \\(U\\) is Gaussian since we assign a conjugate prior on \\(U\\), hence \\[ [U| \\cdot] \\sim \\mathcal{N}(M_U, V_U),\\] where \\[V_U = (\\Delta_2^\\top \\Delta_2 + \\Sigma_S^{-1})^{-1} \\] and \\[M_U = V_U (\\Delta_2^\\top (Z- (D\\beta+ \\Delta_1 S))) \\] Update joint posterior of \\(Z\\) using a Gibbs step. The posterior distribution for \\(Z\\) is truncated Gaussian, hence \\[ [Z| \\cdot] \\sim \\mathcal{TN}(D\\beta+ \\Delta_1 S + \\Delta_2 U, 1, I(Y, Z)),\\] where \\[I(Y, Z) = I(Y=1)I(Z \u0026gt; 0) + I(Y=0)I(Z \\leq 0)\\].   Simulation Study Generating synthetic data #load the required package library(geoR, quietly = TRUE) ## -------------------------------------------------------------- ## Analysis of Geostatistical Data ## For an Introduction to geoR go to http://www.leg.ufpr.br/geoR ## geoR version 1.7-5.2.1 (built on 2016-05-02) is now loaded ## -------------------------------------------------------------- require(Matrix, quietly = TRUE) #simulating my type of data n_add \u0026lt;- 250 n_pat \u0026lt;- 150 n_reg \u0026lt;- 50 adm \u0026lt;- 1:n_add pat \u0026lt;- c(1:n_pat, sample(1:n_pat, n_add-n_pat)) reg \u0026lt;- c(1:n_reg, sample(1:n_reg, n_add-n_reg, replace=TRUE)) data \u0026lt;- data.frame(adm, pat, reg) ##### x \u0026lt;- rnorm(n_add) # Create n x D design matrix D \u0026lt;- 2 # We learn a linear function X \u0026lt;- cbind(rep(1, n_add), x) # True values of regression coeffiecients theta true_theta \u0026lt;- c(-0.5, 0.5) # simulate the gaussian process S \u0026lt;- grf(n=n_reg, grid = \u0026quot;irreg\u0026quot;, cov.pars = c(1, 0.5), xlims = c(0, 6), ylims = c(0, 6)) ## grf: simulation(s) on randomly chosen locations with 50 points ## grf: process with 1 covariance structure(s) ## grf: nugget effect is: tausq= 0 ## grf: covariance model 1 is: exponential(sigmasq=1, phi=0.5) ## grf: decomposition algorithm used is: cholesky ## grf: End of simulation procedure. Number of realizations: 1 xy \u0026lt;- S$coords u \u0026lt;- rnorm(n_pat, sd=sqrt(0.2)) # create the mapping matrix inc_mat_reg \u0026lt;-t(as(as.factor(reg),Class=\u0026quot;sparseMatrix\u0026quot;)) inc_mat_pat \u0026lt;-t(as(as.factor(pat),Class=\u0026quot;sparseMatrix\u0026quot;)) # Obtain the vector with probabilities of success p using the probit link p \u0026lt;- pnorm(as.numeric(X %*% true_theta + inc_mat_reg%*%cbind(S$data) + inc_mat_pat%*%cbind(u))) # Generate binary observation data y y \u0026lt;- cbind(rbinom(n_add, 1, p)) data \u0026lt;- data.frame(data, x, y) head(data) ## adm pat reg x y ## 1 1 1 1 0.5402883 0 ## 2 2 2 2 0.1756355 0 ## 3 3 3 3 1.1559319 1 ## 4 4 4 4 0.8420048 0 ## 5 5 5 5 -0.4327365 0 ## 6 6 6 6 -0.9763709 0  Fiting a non spatial model # fit the non-spatial model fit \u0026lt;- glm(formula = y~x, family = binomial(link = \u0026quot;probit\u0026quot;), data=data) summary(fit) ## ## Call: ## glm(formula = y ~ x, family = binomial(link = \u0026quot;probit\u0026quot;), data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3398 -0.8973 -0.7382 1.3171 2.0473 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -0.46790 0.08364 -5.594 2.22e-08 *** ## x 0.25979 0.08494 3.058 0.00222 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 313.43 on 249 degrees of freedom ## Residual deviance: 303.73 on 248 degrees of freedom ## AIC: 307.73 ## ## Number of Fisher Scoring iterations: 4  Fiting a full bayesian spatial probit model using GRF # Library for sampling from Multivariate Normal distribution require(mvtnorm, quietly = TRUE) # Library for sampling from Truncated Normal distribution require(truncnorm, quietly = TRUE) # Variables that we will need later N1 \u0026lt;- sum(y) # Number of successes N0 \u0026lt;- n_add - N1 # Number of failures # Conjugate prior on the coefficients \\beta ~ N(mu_beta, Sigma_beta) mu_beta \u0026lt;- c(fit$coefficients) # Sigma_beta \u0026lt;- diag(10, 2) Sigma_beta \u0026lt;- as.matrix(vcov(fit)) * 10 # Compute the distance matrix over the region U \u0026lt;- as.matrix(dist(xy)) # proposal covariance prop_Sigma \u0026lt;- matrix(c(0.01, 0, 0, 0.01), 2) prop_Sigma_tau2 \u0026lt;- 0.01 # [,1] [,2] # [1,] 0.14450882 0.03176559 # [2,] 0.03176559 0.34874799 # Initialize parameters re_para \u0026lt;- c(1, 0.5, 0.2) beta \u0026lt;- c(-0.5, 0.5) Sigma_s \u0026lt;-re_para[1]*exp(-U/re_para[2]) S \u0026lt;- rmvnorm(n = 1, mean = rep(0, n_reg), sigma = Sigma_s) u \u0026lt;- rnorm(n = n_pat, mean = 0, sd=sqrt(re_para[3])) mu_z \u0026lt;- X %*% beta + inc_mat_reg%*%as.numeric(S) + inc_mat_pat%*%u z \u0026lt;- rep(0, n_add) z[y == 0] \u0026lt;- rtruncnorm(N0, mean = mu_z[y == 0], sd = 1, a = -Inf, b = 0) z[y == 1] \u0026lt;- rtruncnorm(N1, mean = mu_z[y == 1], sd = 1, a = 0, b = Inf) # Number of simulations for Gibbs sampler N_sim \u0026lt;- 110000 # Burn in period burn_in \u0026lt;- 10000 # Matrix storing samples of the \\beta parameter beta_chain \u0026lt;- matrix(beta, nrow = N_sim, ncol = D) # Matrix storing samples of the latent variable, z z_chain \u0026lt;- matrix(0, nrow = N_sim, ncol = n_add) # Matrix storing samples of the parameter of S and U, sigma^2, phi and tau^2 re_para_chain \u0026lt;- matrix(re_para, nrow = N_sim, ncol = 3) # --------------------------------- # Gibbs sampling algorithm # --------------------------------- # Compute posterior variance of beta prec_beta \u0026lt;- solve(Sigma_beta) V \u0026lt;- solve(prec_beta + crossprod(X, X)) #acceptance probability accept1 \u0026lt;- 0 accept2 \u0026lt;- 0 for (t in 2:N_sim) { # get the random effect parameters sigma2 \u0026lt;- re_para[1] phi \u0026lt;- re_para[2] tau2 \u0026lt;- re_para[3] # update the proposal variance if (t \u0026gt; 1000){ prop_Sigma \u0026lt;- cov(log(re_para_chain[,1:2]))*((2.38)^2)/2 prop_Sigma_tau2 \u0026lt;- cov(log(re_para_chain))[3,3]*((2.38)^2) } # Simulate each of the spatial random effects parameters using metropolis-hasting # the proposal sigma2_phi_proposal \u0026lt;- rmvnorm(n=1, mean=log(c(sigma2, phi)), sigma = prop_Sigma) # the acceptance probability alpha \u0026lt;- exp(dmvnorm(x = as.numeric(S), mean = rep(0, n_reg), sigma=exp(sigma2_phi_proposal[1])*exp(-U/exp(sigma2_phi_proposal[2])), log = TRUE) + dnorm(x = exp(sigma2_phi_proposal[1]), mean = 1, sd = 1.00005, log = TRUE) + #prior of sigma2 dnorm(x= exp(sigma2_phi_proposal[2]), mean = 0.5, sd = 1.00001, log = TRUE) - #prior of phi dmvnorm(x = as.numeric(S), mean = rep(0, n_reg), sigma= Sigma_s, log = TRUE)- dnorm(x = sigma2, mean = 1, sd = 1.00005, log = TRUE) - dnorm(x= phi, mean = 0.5, sd = 1.00001, log = TRUE) + log(exp(sigma2_phi_proposal[1])/sigma2) + log(exp(sigma2_phi_proposal[2])/phi)) if (alpha \u0026gt; runif(1)) { accept1 \u0026lt;- accept1 + 1 sigma2 \u0026lt;- exp(sigma2_phi_proposal[1]) phi \u0026lt;- exp(sigma2_phi_proposal[2]) #cat(\u0026quot;accept =\u0026quot;, c(sigma2,phi), \u0026quot;\\n\u0026quot;) } # Simulate each of the random effects parameters using metropolis-hasting # the likelihood # tau2.lik \u0026lt;- sum(dnorm(x = u, mean = rep(0, n_pat), sd= sqrt(tau2), log = TRUE)) # the proposal tau2_proposal \u0026lt;- rnorm(n=1, mean=log(c(tau2)), sd = sqrt(prop_Sigma_tau2)) # the acceptance probability alpha \u0026lt;- exp(sum(dnorm(x = u, mean = rep(0, n_pat), sd= sqrt(exp(tau2_proposal)), log = TRUE)) + dnorm(x= exp(tau2_proposal), mean = 0.2, sd = 1.0001, log = TRUE) - sum(dnorm(x = u, mean = rep(0, n_pat), sd= sqrt(tau2), log = TRUE)) - dnorm(x= tau2, mean = 0.2, sd = 1.0001, log = TRUE) + log(exp(tau2_proposal)/tau2)) if (alpha \u0026gt; runif(1)) { accept2 \u0026lt;- accept2 + 1 tau2 \u0026lt;- exp(tau2_proposal) #cat(\u0026quot;accept =\u0026quot;, tau2, \u0026quot;\\n\u0026quot;) } ####### # Compute posterior of S Sigma_s \u0026lt;-sigma2*exp(-U/phi) # Compute mean and variance of S V_s \u0026lt;- solve(diag(table(reg)) + solve(Sigma_s)) mu_s \u0026lt;- V_s%*%(crossprod(inc_mat_reg, (z-(X%*%beta + inc_mat_pat%*%u)))) S \u0026lt;- t(rmvnorm(n = 1, mean = mu_s, sigma = as.matrix(V_s))) # Compute posterior of u # Compute mean and variance of u V_u \u0026lt;- solve(diag(table(pat)) + diag(1/tau2, n_pat)) mu_u \u0026lt;- V_u%*%(crossprod(inc_mat_pat, (z-(X%*%beta + inc_mat_reg%*%S)))) u \u0026lt;- t(rmvnorm(n = 1, mean = mu_u, sigma = as.matrix(V_u))) # Compute posterior mean of beta M \u0026lt;- V %*% (prec_beta %*% mu_beta + crossprod(X, (z-(inc_mat_reg%*%as.numeric(S) + inc_mat_pat%*%u)))) # M \u0026lt;- V %*% (prec_beta %*% mu_beta + crossprod(X, (z))) # Draw variable \\theta from its full conditional: \\beta | z, X beta \u0026lt;- c(rmvnorm(1, M, as.matrix(V))) # Update Mean of z mu_z \u0026lt;- X %*% beta + inc_mat_reg%*%as.numeric(S) + inc_mat_pat%*%u # Draw latent variable z from its full conditional: z | \\theta, y, X z[y == 0] \u0026lt;- rtruncnorm(N0, mean = mu_z[y == 0], sd = 1, a = -Inf, b = 0) z[y == 1] \u0026lt;- rtruncnorm(N1, mean = mu_z[y == 1], sd = 1, a = 0, b = Inf) # Store the \\theta draws beta_chain[t, ] \u0026lt;- beta # Store the z draws z_chain[t, ] \u0026lt;- z # Store the covariance parameters draws re_para[1] \u0026lt;- sigma2 re_para[2] \u0026lt;- phi re_para[3] \u0026lt;- tau2 re_para_chain[t, ] \u0026lt;- re_para #cat(\u0026quot;iter =\u0026quot;, t, \u0026quot;\\n\u0026quot;) } post_beta \u0026lt;- colMeans(beta_chain[-(1:burn_in), ]) post_beta ## [1] -0.5684900 0.3836226 post_rf \u0026lt;- colMeans(re_para_chain[-(1:burn_in), ]) post_rf ## [1] 1.4133263 1.0859733 0.3904089 Ploting the diagnostic of the inference #load the package require(ggmcmc, quietly = TRUE) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union ## ## Attaching package: \u0026#39;tidyr\u0026#39; ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## expand require(coda, quietly = TRUE) df \u0026lt;- data.frame(beta_chain, re_para_chain) colnames(df) \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;, \u0026quot;sigma2\u0026quot;, \u0026quot;phi\u0026quot;, \u0026quot;tau2\u0026quot;) df \u0026lt;- df[-1,] df \u0026lt;- df[-(1:(burn_in-1)),] #burn in df \u0026lt;- df[seq(1, nrow(df), 100),] #thining bbb \u0026lt;- ggs(mcmc(df)) # ggmcmc(bbb) ggs_traceplot(bbb) ggs_density(bbb) ggs_autocorrelation(bbb) # compute the acceptance rate accept1/N_sim # for sigma2 and phi ## [1] 0.09990909 accept2/N_sim # for tau2 ## [1] 0.07552727    References Albert, James H, and Siddhartha Chib. 1993. “Bayesian Analysis of Binary and Polychotomous Response Data.” Journal of the American Statistical Association 88 (422). Taylor \u0026amp; Francis Group: 669–79.\n Besag, Julian, Jeremy York, and Annie Mollié. 1991. “Bayesian image restoration, with two applications in spatial statistics.” Annals of the Institute of Statistical Mathematics 43 (1): 1–20. https://econpapers.repec.org/RePEc:spr:aistmt✌️43:y:1991:i:1:p:1-20.\n Brooks, Steve, Andrew Gelman, Galin Jones, and Xiao-Li Meng. 2011. Handbook of Markov Chain Monte Carlo. CRC press.\n LeSage, James P. 2000. “Bayesian Estimation of Limited Dependent Variable Spatial Autoregressive Models.” Geographical Analysis 32 (1). Wiley Online Library: 19–35.\n LeSage, James, and Robert Kelley Pace. 2009. Introduction to Spatial Econometrics. Chapman; Hall/CRC.\n Smith, Tony E, and James P LeSage. 2004. “A Bayesian Probit Model with Spatial Dependencies.” In Spatial and Spatiotemporal Econometrics, 127–60. Emerald Group Publishing Limited.\n   ","date":1552521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552521600,"objectID":"904b8eaff6d695cf367af41f9659ebd4","permalink":"/post/probit_model/probit_modelling_markdown/","publishdate":"2019-03-14T00:00:00Z","relpermalink":"/post/probit_model/probit_modelling_markdown/","section":"post","summary":"Introduction Spatial probit models is very popular in spatial econometrics and the book of J. LeSage and Pace (2009) gives a very good overview. This is basically an extension of probit model when one is interested to adjust for both fixed and spatial random effect.","tags":null,"title":"Spatial Probit Model Using Gaussian Random Field","type":"post"},{"authors":null,"categories":null,"content":" Introduction This tutorial simply estimate the parameter of a geostatistical model using the TensorFlow API from R. There are many tutorial and links online on how to use TensorFlow in R, see https://www.r-bloggers.com/an-introduction-to-tensorflow/ for an introduction to TensorFlow and https://kyleclo.github.io/maximum-likelihood-in-tensorflow-pt-1/ for TensorFlow’s autodifferentiation toolbox for maximum likelihood estimation.\nTensorFlow is popularly used machine learning and deep learning. It is very essential that statisticians begin to benefit from its features. Some of the features that can be useful are:\n autodifferentiation. It automatically suports Multicore CPU and GPU. It can be used from R. I will show this very soon. Graphical probabilistic modelling is very possible with the use of TensorFlow Probability. One can monitor the learning process through the TensorBoard.  The section that follows gives a general description typical geostatistical model.\nLinear geostatistical model Let \\(y_i\\) for \\(i = 1, \\ldots, n\\) denote measurements at locations \\(x_i \\in A \\subset \\mathcal{R}^2\\) , where \\(A\\) corresponds to the area of interest. Let \\(S(x)\\) and \\(Z(x)\\) denote a stationary zero-mean Gaussian process and Gaussian noise, respectively. The standard linear geostatsitical model for the data can then be expressed as \\[Y(x_i) = d(x_i)^\\top \\beta + S(x_i) + Z(x_i)\\] where \\(Y(x_i)\\) is the random variable associated with the data \\(y(x_i)\\) , and \\(d(x_i)\\) is a vector of the covariates with corresponding vector of regression coefficients \\(\\beta\\). We also assume that the stationary process \\(S(x)\\) is isotropic, \\(cov{S(x), S(x 0 )} = \\sigma^2 \\rho(u; \\phi),\\) where \\(\\sigma^2\\) is the variance of \\(S(x)\\), \\(\\rho(\\cdot; \\phi)\\) is a correlation function with parameter \\(\\phi\\) (assumed to be exponential in this case) and \\(u\\) is the Euclidean distance between \\(x\\) and \\(x\u0026#39;\\) . Finally, we use \\(\\tau^2\\) to denote the variance of the Gaussian noise \\(Z(x_i)\\). \\(\\theta = (\\beta, \\sigma^2, \\phi, \\tau^2)\\) is the vector of the parameter in the model.\n  Geostatistics with TensorFlow Simulating the data\nn \u0026lt;- 625 # number of data point x \u0026lt;- rnorm(n) # simulating a covariate ### generate the data location xy \u0026lt;- expand.grid(x=seq(0, 3, length.out = sqrt(n)), y=seq(0, 3, length.out = sqrt(n))) d \u0026lt;- as.matrix(dist(xy)) # compute the distance matrix # The parameters of the model beta0 \u0026lt;- 0.5 beta1 \u0026lt;- 2.5 sigma2 \u0026lt;- 1 phi \u0026lt;- 0.3 tau2 \u0026lt;- 0.1 # Generate the covariance matrix of the process Sigma \u0026lt;- sigma2*exp(-d/phi) S \u0026lt;- t(chol(Sigma))%*%rnorm(n) # Finally simulate the data y \u0026lt;- as.numeric(beta0 + beta1 *x + S + rnorm(n = n, mean = 0, sd = sqrt(tau2))) Loading the R packages\n#Load the package library(tensorflow) tfp \u0026lt;- import(\u0026quot;tensorflow_probability\u0026quot;) # import the tensorflow probability tfd \u0026lt;- tfp$distributions # get the tensorflow distributions Defining the model\nsess \u0026lt;- tf$Session() x_data \u0026lt;- tf$placeholder(dtype = \u0026quot;float\u0026quot;, shape = n, name = \u0026quot;x\u0026quot;) # Placeholder for Petal.Length y_data \u0026lt;- tf$placeholder(dtype = \u0026quot;float\u0026quot;, shape = n, name = \u0026quot;y\u0026quot;) # Placeholder for Petal.Width d_mat \u0026lt;- tf$placeholder(dtype = tf$float32, name=\u0026quot;d\u0026quot;) beta0 \u0026lt;- tf$Variable(0.1, name = \u0026quot;Intercept\u0026quot;) beta1 \u0026lt;- tf$Variable(0.1, name = \u0026quot;coefficient\u0026quot;) #initialise the parameters sigma2 \u0026lt;- tf$Variable(0.1, name = \u0026quot;sigma2\u0026quot;) phi \u0026lt;- tf$Variable(0.1, name = \u0026quot;phi\u0026quot;) tau2 \u0026lt;- tf$Variable(0.1, name = \u0026quot;tau2\u0026quot;) # The expectation of the model y_hat \u0026lt;- beta0 + beta1 * x_data  Maximum Likelihood Estimation\n# Setting up the model n \u0026lt;- tf$constant(n) # define a Gaussian distribution with mean = y_hat and covariance matrix using an exponential covariance function gaussian_dist \u0026lt;- tfd$MultivariateNormalFullCovariance(loc = y_hat, covariance_matrix = sigma2*tf$exp(-d_mat/phi) + tf$eye(n)*tau2) # log_likelihood (y_data | theta) log_prob \u0026lt;- gaussian_dist$log_prob(value = y_data) # negative_log_likelihood (y_data | theta) neg_log_likelihood \u0026lt;- -1.0 * tf$reduce_sum(log_prob) # gradient of neg_log_likelihood wrt (theta) grad \u0026lt;- tf$gradients(neg_log_likelihood,c(beta0, beta1, sigma2, phi, tau2)) # optimizer optimizer \u0026lt;- tf$train$AdamOptimizer(learning_rate = 0.01) train_op \u0026lt;- optimizer$minimize(loss = neg_log_likelihood) Creating an interface of the tensorboard\nMSE_hist \u0026lt;- tf$summary$scalar(\u0026quot;neg_log_likelihood\u0026quot;, neg_log_likelihood) # save all values of neg_log_likelihood beta0_hist \u0026lt;- tf$summary$scalar(\u0026quot;Intercept\u0026quot;, beta0) beta1_hist \u0026lt;- tf$summary$scalar(\u0026quot;Coefficient\u0026quot;, beta1) sigma2_hist \u0026lt;- tf$summary$scalar(\u0026quot;sigma2\u0026quot;, sigma2) phi_hist \u0026lt;- tf$summary$scalar(\u0026quot;phi\u0026quot;, phi) tau2_hist \u0026lt;- tf$summary$scalar(\u0026quot;tau2\u0026quot;, tau2) merged \u0026lt;- tf$summary$merge_all() # Merges all summaries collected in the default graph. train_writer \u0026lt;- tf$summary$FileWriter(logdir = \u0026quot;logs\u0026quot;) train_writer$add_graph(sess$graph) # add a graph structure Running the session\nsess$run(tf$global_variables_initializer()) for (epoch in 1:200) { result \u0026lt;- sess$run(list(merged, # the merged parameters and train_op, # Min neg_log_likelihood neg_log_likelihood, # neg_log_likelihood grad), # Gradient feed_dict = dict(x_data = x, y_data = y, d_mat=d)) summary \u0026lt;- result[[1]] # extract the summary result of merged train_writer$add_summary(summary, epoch) # write summary to disk } #print the maximum likelihood estimate of the parameters cat(\u0026quot;Intercept: \u0026quot;, sess$run(beta0), \u0026quot;\\n Coefficient: \u0026quot;, sess$run(beta1), \u0026quot;\\n sigma2: \u0026quot;, sess$run(sigma2), \u0026quot;\\n phi: \u0026quot;, sess$run(phi), \u0026quot;\\n tau2: \u0026quot;, sess$run(tau2), \u0026quot;\\n\u0026quot;) ## Intercept: 0.2836692 ## Coefficient: 1.201712 ## sigma2: 0.5700233 ## phi: 0.110626 ## tau2: 0.5315924 # print the gradient of the log-likelihood with respect to the parameters cat(\u0026quot;Gradient wrt: d.beta0 \u0026quot;, result[[4]][[1]], \u0026quot;\\n d.beta1: \u0026quot;, result[[4]][[2]], \u0026quot;\\n d.sigma2: \u0026quot;, result[[4]][[3]], \u0026quot;\\n d.phi: \u0026quot;, result[[4]][[4]], \u0026quot;\\n d.tau2: \u0026quot;, result[[4]][[5]], \u0026quot; \\n\u0026quot;) ## Gradient wrt: d.beta0 -3.351908 ## d.beta1: -768.9117 ## d.sigma2: -328.6408 ## d.phi: 190.2612 ## d.tau2: -392.9058 sess$close() tf$reset_default_graph() Monitoring with TensorBoard\ntensorboard(log_dir = \u0026quot;logs\u0026quot;) # Play with tensorboard ## Started TensorBoard at http://127.0.0.1:3902  ","date":1551484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551484800,"objectID":"7aba5a7302ef89a89a29dfeeabd5b34b","permalink":"/post/geostatisticswithtensorflow/geostatistics_mle_tf/","publishdate":"2019-03-02T00:00:00Z","relpermalink":"/post/geostatisticswithtensorflow/geostatistics_mle_tf/","section":"post","summary":"Introduction This tutorial simply estimate the parameter of a geostatistical model using the TensorFlow API from R. There are many tutorial and links online on how to use TensorFlow in R, see https://www.","tags":null,"title":"Fitting Geostatistical Model Using TensorFlow API from R","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588353970,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":[],"categories":null,"content":"","date":1529067600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529067600,"objectID":"df7390d414c5991c2532101bd2b7676b","permalink":"/talk/toronto/","publishdate":"2020-01-28T13:00:00Z","relpermalink":"/talk/toronto/","section":"talk","summary":"n this paper, we develop a computationally efficient discrete approximation to log‐Gaussian Cox process (LGCP) models for the analysis of spatially aggregated disease count data. Our approach overcomes an inherent limitation of spatial models based on Markov structures, namely, that each such model is tied to a specific partition of the study area, and allows for spatially continuous prediction. We compare the predictive performance of our modelling approach with LGCP through a simulation study and an application to primary biliary cirrhosis incidence data in Newcastle upon Tyne, UK. Our results suggest that, when disease risk is assumed to be a spatially continuous process, the proposed approximation to LGCP provides reliable estimates of disease risk both on spatially continuous and aggregated scales. The proposed methodology is implemented in the open‐source R package SDALGCP.","tags":[],"title":"A Spatially Discrete Approximation to Log-Gaussian Cox Processes for Modelling Aggregated Disease Count Data","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588353970,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588353970,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["FB Adebola","OO Johnson"],"categories":null,"content":"","date":1441062000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441062000,"objectID":"5c16ea2bbdf7b529a908cc2aaaac21a2","permalink":"/publication/warner2/","publishdate":"2015-09-01T00:00:00+01:00","relpermalink":"/publication/warner2/","section":"publication","summary":"This paper presents a modification of Warner’s [8] Randomized Response model. According to O’Muircheartaigh et al [7], non-response is inevitable in a survey; in view of this, our model further reduces the non-response bias by further sampling for the non-respondents. In this paper we performed an empirical practice of our model and we also performed the empirical comparison of our model with Warner [8] model. We discovered that our model is more efficient than the Warner [8] model.","tags":[],"title":"An improved Warner’s randomized response model","type":"publication"},{"authors":["FB Adebola","OO Johnson","NA Adegoke"],"categories":null,"content":"","date":1409526000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409526000,"objectID":"c7fd962f06891b109c5d4183cfe5725f","permalink":"/publication/warner1/","publishdate":"2014-09-01T00:00:00+01:00","relpermalink":"/publication/warner1/","section":"publication","summary":"This paper presents a modification of Kim and Warde’s (2004) Stratified Warner’s Randomized Response Technique (SWRRT) and Kim and Elam’s (2007) Stratified Unrelated Question Randomized Response Technique with known distribution (SURRT) using optimal allocation. Our models further reduce the non-response bias by introducing the concept of sub-samples of non-respondent developed by Hansen and Hurwitz (1946) to the above mentioned models. In this paper we perform an empirical practice of our model and we also perform the empirical comparison of our models with both Kim and Warde (2004) and Kim and Elam (2007) models. We discovered a new note in the empirical comparative study of our models.","tags":[],"title":"A modified stratified randomized response technique","type":"publication"}]